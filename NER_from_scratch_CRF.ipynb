{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_from_scratch_CRF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMU9/Mb6Wbnq4HFbpY6TwS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbansen3027/NER_from_scratch/blob/main/NER_from_scratch_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPwSFhYaghw"
      },
      "source": [
        "Conditional Random Fields\n",
        "CRFs are a type of discriminative undirected probabilistic graphical model.whose nodes can be divided into exactly two disjoint sets X and Y, the observed and output variables, respectively; the conditional distribution p(Y|X) is then modeled.\n",
        "Learning the parameters theta  is usually done by maximum likelihood learning for p(Y_i|X_i; \\theta). If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex. It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm.\n",
        "\n",
        "CoNLL dataset \n",
        "\n",
        "BIO notation: \n",
        "* B indicates the beginning of an entity;\n",
        "* I inside an entity, indicates when entities comprise more than one word;\n",
        "* O other, indicates non-\n",
        "entities.\n",
        "\n",
        "Models trained on Wikipedia corpus (Nothman et al., 2013) use a less fine-grained NER annotation scheme and recognise the following entities:\n",
        "\n",
        "PER\tNamed person or family.\n",
        "LOC\tName of politically or geographically defined location (cities, provinces, countries, international regions, bodies of water, mountains).\n",
        "ORG\tNamed corporate, governmental, or other organizational entity.\n",
        "MISC\tMiscellaneous entities, e.g. events, nationalities, products or works of art."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NewsvVvldLSw",
        "outputId": "7972f400-5bbc-4bf0-b17f-cd1e602d4b03"
      },
      "source": [
        "! head /content/train.txt"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EU\tB-ORG\n",
            "rejects\tO\n",
            "German\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "British\tB-MISC\n",
            "lamb\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_sC3gaNBqHv"
      },
      "source": [
        "! pip install sklearn_crfsuite -q"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkRCJYxmkD3e",
        "outputId": "d266af38-e13f-48ed-cf3c-b80f66a420a3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "from nltk.tag import pos_tag\n",
        "from sklearn_crfsuite import CRF, metrics"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwa22sjQdF-5"
      },
      "source": [
        "\"\"\"\n",
        "Load the training/testing data. \n",
        "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
        "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
        "\"\"\"\n",
        "def load_data_conll(file_path):\n",
        "  myoutput, words, tags = [], [], []\n",
        "  fh = open(file_path)\n",
        "  for line in fh:\n",
        "    line = line.strip()\n",
        "    if '\\t' not in line:\n",
        "      #Sentence Ended\n",
        "      myoutput.append([words, tags])\n",
        "      words, tags = [], []\n",
        "    else:\n",
        "      word, tag = line.split('\\t')\n",
        "      words.append(word)\n",
        "      tags.append(tag)\n",
        "  fh.close()\n",
        "  return myoutput"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wi77zSzjpwl"
      },
      "source": [
        "\"\"\"\n",
        "Get features for all words in the sentence\n",
        "Features:\n",
        "- word context: a window of 2 words on either side of the current word, and current word.\n",
        "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n",
        "input: sentence as a list of tokens.\n",
        "output: list of dictionaries. each dict represents features for that word.\n",
        "\"\"\"\n",
        "def sent2feats(sentence):\n",
        "  feats = []\n",
        "  sent_tags = pos_tag(sentence)\n",
        "  # [('John', 'NNP'),(\"'s\", 'POS'),...]\n",
        "  for i in range(len(sentence)):\n",
        "    word = sentence[i]\n",
        "    #word features: word, prev 2 words, next 2 words in the sentence.\n",
        "    #POS tag features: current tag, previous and next 2 tags.\n",
        "    word_feats = {}\n",
        "    word_feats['word'] = word\n",
        "    word_feats['tag'] = sent_tags[i][1]\n",
        "    if i==0:\n",
        "      word_feats['prevWord'] = '<S>'\n",
        "      word_feats['prevSecondWord'] = '<S>'\n",
        "      word_feats['prevTag'] = '<S>'\n",
        "      word_feats['prevSecondTag'] = '<S>'\n",
        "    elif i == 1:\n",
        "      word_feats['prevWord'] = sentence[i-1]\n",
        "      word_feats['prevSecondWord'] = '<S>'\n",
        "      word_feats['prevTag'] = sent_tags[i-1][1]\n",
        "      word_feats['prevSecondTag'] = '<S>'\n",
        "    else:\n",
        "      word_feats['prevWord'] = sentence[i-1]\n",
        "      word_feats['prevSecondWord'] = sentence[i-2]\n",
        "      word_feats['prevTag'] = sent_tags[i-1][1]\n",
        "      word_feats['prevSecondTag'] = sent_tags[i-2][1]\n",
        "    if i == len(sentence) - 1:\n",
        "      word_feats['nextWord'] = '</S>'\n",
        "      word_feats['nextNextWord'] = '</S>'\n",
        "      word_feats['nextTag'] = '</S>'\n",
        "      word_feats['nextNextTag'] = '</S>'\n",
        "    elif i == len(sentence) - 2:\n",
        "      word_feats['nextWord'] = sentence[i+1]\n",
        "      word_feats['nextNextWord'] = '</S>'\n",
        "      word_feats['nextTag'] = sent_tags[i+1][1]\n",
        "      word_feats['nextNextTag'] = '</S>'\n",
        "    else:\n",
        "      word_feats['nextWord'] = sentence[i+1]\n",
        "      word_feats['nextNextWord'] = sentence[i+2]\n",
        "      word_feats['nextTag'] = sent_tags[i+1][1]\n",
        "      word_feats['nextNextTag'] = sent_tags[i+2][1]\n",
        "   \n",
        "    feats.append(word_feats)\n",
        "  return feats "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmROO73YiXD_"
      },
      "source": [
        "#Extract features from the conll data, after loading it.\n",
        "def get_features_conll(conll_data):\n",
        "  feats, labels = [], []\n",
        "  for sentence in conll_data:\n",
        "    feats.append(sent2feats(sentence[0]))\n",
        "    labels.append(sentence[1])\n",
        "  return feats, labels"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDHLKPpBxyR5"
      },
      "source": [
        "def train_seq(feats, labels, feats_val, labels_val):\n",
        "  crf = CRF(algorithm = 'lbfgs', c1 = 0.1, c2 = 10, max_iterations = 50)\n",
        "  crf.fit(feats, labels)\n",
        "  labels = list(crf.classes_)\n",
        "  labels_val_pred = crf.predict(feats_val)\n",
        "  print(metrics.flat_f1_score(labels_val, labels_val_pred, average = 'weighted', labels = labels))\n",
        "  print(metrics.flat_classification_report(labels_val, labels_val_pred, labels = labels, digits = 3))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXVrwV_6Z-Kk",
        "outputId": "7efd614f-24a4-4b7d-d354-41555bbfe7ee"
      },
      "source": [
        "def main():\n",
        "  train_path = 'train.txt'\n",
        "  val_path = 'val.txt'\n",
        "  conll_train = load_data_conll(train_path)\n",
        "  conll_val = load_data_conll(val_path)\n",
        "  print(conll_train[:2])\n",
        "  feats, labels = get_features_conll(conll_train)\n",
        "  feats_val, labels_val = get_features_conll(conll_val)\n",
        "  print(feats[:2])\n",
        "  train_seq(feats, labels, feats_val, labels_val)\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']], [['Peter', 'Blackburn'], ['B-PER', 'I-PER']]]\n",
            "[[{'word': 'EU', 'tag': 'NNP', 'prevWord': '<S>', 'prevSecondWord': '<S>', 'prevTag': '<S>', 'prevSecondTag': '<S>', 'nextWord': 'rejects', 'nextNextWord': 'German', 'nextTag': 'VBZ', 'nextNextTag': 'JJ'}, {'word': 'rejects', 'tag': 'VBZ', 'prevWord': 'EU', 'prevSecondWord': '<S>', 'prevTag': 'NNP', 'prevSecondTag': '<S>', 'nextWord': 'German', 'nextNextWord': 'call', 'nextTag': 'JJ', 'nextNextTag': 'NN'}, {'word': 'German', 'tag': 'JJ', 'prevWord': 'rejects', 'prevSecondWord': 'EU', 'prevTag': 'VBZ', 'prevSecondTag': 'NNP', 'nextWord': 'call', 'nextNextWord': 'to', 'nextTag': 'NN', 'nextNextTag': 'TO'}, {'word': 'call', 'tag': 'NN', 'prevWord': 'German', 'prevSecondWord': 'rejects', 'prevTag': 'JJ', 'prevSecondTag': 'VBZ', 'nextWord': 'to', 'nextNextWord': 'boycott', 'nextTag': 'TO', 'nextNextTag': 'VB'}, {'word': 'to', 'tag': 'TO', 'prevWord': 'call', 'prevSecondWord': 'German', 'prevTag': 'NN', 'prevSecondTag': 'JJ', 'nextWord': 'boycott', 'nextNextWord': 'British', 'nextTag': 'VB', 'nextNextTag': 'JJ'}, {'word': 'boycott', 'tag': 'VB', 'prevWord': 'to', 'prevSecondWord': 'call', 'prevTag': 'TO', 'prevSecondTag': 'NN', 'nextWord': 'British', 'nextNextWord': 'lamb', 'nextTag': 'JJ', 'nextNextTag': 'NN'}, {'word': 'British', 'tag': 'JJ', 'prevWord': 'boycott', 'prevSecondWord': 'to', 'prevTag': 'VB', 'prevSecondTag': 'TO', 'nextWord': 'lamb', 'nextNextWord': '.', 'nextTag': 'NN', 'nextNextTag': '.'}, {'word': 'lamb', 'tag': 'NN', 'prevWord': 'British', 'prevSecondWord': 'boycott', 'prevTag': 'JJ', 'prevSecondTag': 'VB', 'nextWord': '.', 'nextNextWord': '</S>', 'nextTag': '.', 'nextNextTag': '</S>'}, {'word': '.', 'tag': '.', 'prevWord': 'lamb', 'prevSecondWord': 'British', 'prevTag': 'NN', 'prevSecondTag': 'JJ', 'nextWord': '</S>', 'nextNextWord': '</S>', 'nextTag': '</S>', 'nextNextTag': '</S>'}], [{'word': 'Peter', 'tag': 'NNP', 'prevWord': '<S>', 'prevSecondWord': '<S>', 'prevTag': '<S>', 'prevSecondTag': '<S>', 'nextWord': 'Blackburn', 'nextNextWord': '</S>', 'nextTag': 'NNP', 'nextNextTag': '</S>'}, {'word': 'Blackburn', 'tag': 'NNP', 'prevWord': 'Peter', 'prevSecondWord': '<S>', 'prevTag': 'NNP', 'prevSecondTag': '<S>', 'nextWord': '</S>', 'nextNextWord': '</S>', 'nextTag': '</S>', 'nextNextTag': '</S>'}]]\n",
            "0.9254909683914324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ORG      0.674     0.559     0.611      1661\n",
            "           O      0.973     0.981     0.977     38323\n",
            "      B-MISC      0.643     0.308     0.416       702\n",
            "       B-PER      0.766     0.772     0.769      1617\n",
            "       I-PER      0.813     0.890     0.850      1156\n",
            "       B-LOC      0.706     0.759     0.732      1668\n",
            "       I-ORG      0.559     0.701     0.622       835\n",
            "      I-MISC      0.632     0.500     0.558       216\n",
            "       I-LOC      0.756     0.482     0.589       257\n",
            "\n",
            "    accuracy                          0.928     46435\n",
            "   macro avg      0.725     0.661     0.680     46435\n",
            "weighted avg      0.926     0.928     0.925     46435\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}